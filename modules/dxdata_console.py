import requests
import json
import os
import copy
from datetime import datetime
from modules.config_loader import MAIMAI_VERSION, DXDATA_VERSION_FILE

def merge_json(source, target):
    if isinstance(source, dict) and isinstance(target, dict):
        for key, value in source.items():
            if key not in target:
                target[key] = copy.deepcopy(value)
            else:
                # ç©ºå­—ç¬¦ä¸²é€»è¾‘
                if isinstance(value, str) and isinstance(target[key], str):
                    if value and not target[key]:
                        target[key] = value
                    elif target[key] and not value:
                        pass  # ä¿ç•™ target çš„éç©ºå€¼

                # ç©ºåˆ—è¡¨é€»è¾‘
                elif isinstance(value, list) and isinstance(target[key], list):
                    if value and not target[key]:
                        target[key] = copy.deepcopy(value)
                    elif target[key] and not value:
                        pass  # ä¿ç•™ target çš„éç©ºåˆ—è¡¨
                    else:
                        # éƒ½æœ‰å†…å®¹ -> é€’å½’åˆå¹¶
                        for i in range(min(len(value), len(target[key]))):
                            merge_json(value[i], target[key][i])

                else:
                    # å…¶ä»–ç±»å‹é€’å½’
                    merge_json(value, target[key])

    elif isinstance(source, list) and isinstance(target, list):
        # é¡¶å±‚åˆ—è¡¨åˆå¹¶ï¼šå…ˆåšäº’è¡¥ï¼Œå†é€’å½’
        if source and not target:
            target.extend(copy.deepcopy(source))
        elif target and not source:
            pass
        else:
            for i in range(min(len(source), len(target))):
                merge_json(source[i], target[i])

    return target

def merge_json_by_key(source, target, key_field):
    """
    å°† source å’Œ target ä¸­æŒ‡å®š key_fieldï¼ˆå¦‚ "title"ï¼‰ç›¸åŒçš„å¯¹è±¡åˆå¹¶ã€‚
    å‡è®¾ source/target æ˜¯ç»“æ„ä¸€è‡´çš„ JSONï¼ˆé¡¶å±‚ä¸º dictï¼‰ï¼Œä¸” key_field å¯¹åº”çš„æ˜¯ list[dict]
    """
    result = copy.deepcopy(target)

    for k, v in source.items():
        if isinstance(v, list) and all(isinstance(i, dict) for i in v):
            # æ˜¯ä¸€ä¸ªå¯¹è±¡åˆ—è¡¨ï¼Œå‡†å¤‡æŒ‰ key_field åŒ¹é…
            target_list = result.get(k, [])
            target_index = {item.get(key_field): item for item in target_list}

            for item in v:
                key = item.get(key_field)
                if key in target_index:
                    merge_json(item, target_index[key])
                else:
                    target_list.append(copy.deepcopy(item))
            result[k] = target_list
        else:
            # é»˜è®¤ä½¿ç”¨é€’å½’åˆå¹¶
            if k in result:
                merge_json(v, result[k])
            else:
                result[k] = copy.deepcopy(v)

    return result

def load_dxdata(url):
    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()

        data['songs'] = _split_song_sheets_by_type(data['songs'])
        for song in data['songs']:
            for version in data['versions']:
                if version['version'] == song['version']:
                    for sheet in song.get("sheets", []):
                        if 'count' not in version:
                            version['count'] = 0
                        if sheet['regions']['jp']:
                            version['count'] += 1

        return data

    except requests.RequestException as e:
        return None
    except json.JSONDecodeError as e:
        return None

def _split_song_sheets_by_type(song_list):
    result = []

    for song in song_list:
        base_info = {
            "category": song["category"],
            "title": song["title"],
            "artist": song["artist"],
            "bpm": song["bpm"],
            "cover_url": f"https://shama.dxrating.net/images/cover/v2/{song['imageName'].replace('.png', '')}.jpg",
            "search_acronyms": song.get("searchAcronyms", [])
        }

        sheets_by_type = {"dx": [], "std": [], "utage": []}
        version_by_type = {}

        for sheet in song.get("sheets", []):
            sheet_type = sheet.get("type")
            if "multiverInternalLevelValue" in sheet:
                sheet["internalLevelValue"] = sheet["multiverInternalLevelValue"].get(MAIMAI_VERSION["jp"][-1], sheet["internalLevelValue"])

            if sheet_type in sheets_by_type:
                new_sheet = sheet.copy()
                version_by_type[sheet_type] = new_sheet.pop("version", "")
                new_sheet.pop("type", None)
                sheets_by_type[sheet_type].append(new_sheet)

        for sheet_type, sheets in sheets_by_type.items():
            if sheets:
                entry = base_info.copy()
                entry["type"] = sheet_type
                entry["version"] = version_by_type.get(sheet_type, "")
                entry["sheets"] = sheets
                result.append(entry)

    return result


def get_dxdata_stats(data):
    """
    è·å– dxdata çš„ç»Ÿè®¡ä¿¡æ¯

    Args:
        data: dxdata JSON æ•°æ®

    Returns:
        dict: åŒ…å«æ­Œæ›²æ•°ã€è°±é¢æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯
    """
    if not data or 'songs' not in data:
        return None

    total_songs = len(data['songs'])
    total_sheets = 0

    for song in data['songs']:
        if 'sheets' in song:
            total_sheets += len(song['sheets'])

    return {
        'total_songs': total_songs,
        'total_sheets': total_sheets,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }


def load_dxdata_version_history():
    """åŠ è½½ dxdata ç‰ˆæœ¬å†å²"""
    if not os.path.exists(DXDATA_VERSION_FILE):
        return None

    try:
        with open(DXDATA_VERSION_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return None


def save_dxdata_version_history(stats):
    """ä¿å­˜ dxdata ç‰ˆæœ¬å†å²"""
    try:
        # ç¡®ä¿ data ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(DXDATA_VERSION_FILE), exist_ok=True)

        with open(DXDATA_VERSION_FILE, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        return True
    except Exception:
        return False


def update_dxdata_with_comparison(urls, save_to: str = None):
    """
    æ›´æ–° dxdata å¹¶è¿”å›ä¸ä¸Šæ¬¡çš„å¯¹æ¯”ä¿¡æ¯

    Args:
        url: dxdata API URL
        save_to: ä¿å­˜æ–‡ä»¶è·¯å¾„

    Returns:
        dict: åŒ…å«æ›´æ–°ç»“æœå’Œå¯¹æ¯”ä¿¡æ¯
            {
                'success': bool,
                'new_stats': dict,
                'old_stats': dict,
                'diff': {
                    'songs_added': int,
                    'sheets_added': int
                },
                'message': str
            }
    """
    # åŠ è½½æ—§ç‰ˆæœ¬ä¿¡æ¯
    old_version = load_dxdata_version_history()

    # åŠ è½½æ–°æ•°æ®
    new_datas = []
    for url in urls:
        new_datas.append(load_dxdata(url))
    
    for i in range(1, len(new_datas)):
        new_datas[0] = merge_json_by_key(new_datas[i], new_datas[0], "title")

    new_data = new_datas[0]

    if not new_data:
        return {
            'success': False,
            'message': 'âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ï¼'
        }


    if save_to:
        with open(save_to, "w", encoding="utf-8") as file:
            json.dump(new_data, file, ensure_ascii=False, indent=2)

    # è·å–æ–°æ•°æ®ç»Ÿè®¡
    new_stats = get_dxdata_stats(new_data)

    if not new_stats:
        return {
            'success': False,
            'message': 'âŒ ãƒ‡ãƒ¼ã‚¿è§£æå¤±æ•—ï¼'
        }

    # ä¿å­˜æ–°ç‰ˆæœ¬ä¿¡æ¯
    save_dxdata_version_history(new_stats)

    # è®¡ç®—å·®å¼‚
    if old_version:
        songs_diff = new_stats['total_songs'] - old_version['total_songs']
        sheets_diff = new_stats['total_sheets'] - old_version['total_sheets']

        # æ„å»ºæ¶ˆæ¯
        message_parts = ['âœ… Dxdata Updated!', '']

        if songs_diff > 0:
            message_parts.append(f'ğŸµ æ–°æ›²: +{songs_diff}é¦–')
        elif songs_diff < 0:
            message_parts.append(f'ğŸµ æ¥½æ›²: {songs_diff}é¦–')
        else:
            message_parts.append('ğŸµ æ–°æ›²: ãªã—')

        if sheets_diff > 0:
            message_parts.append(f'ğŸ“Š æ–°è­œé¢: +{sheets_diff}å€‹')
        elif sheets_diff < 0:
            message_parts.append(f'ğŸ“Š è­œé¢: {sheets_diff}å€‹')
        else:
            message_parts.append('ğŸ“Š æ–°è­œé¢: ãªã—')

        message_parts.append('')
        message_parts.append(f'ğŸ“… å‰å›æ›´æ–°: {old_version["timestamp"]}')
        message_parts.append(f'ğŸ“ˆ ç¾åœ¨: æ¥½æ›²{new_stats["total_songs"]}é¦– / è­œé¢{new_stats["total_sheets"]}å€‹')

        return {
            'success': True,
            'new_stats': new_stats,
            'old_stats': old_version,
            'diff': {
                'songs_added': songs_diff,
                'sheets_added': sheets_diff
            },
            'message': '\n'.join(message_parts)
        }
    else:
        # ç¬¬ä¸€æ¬¡æ›´æ–°
        message_parts = [
            'âœ… Dxdata Updated!',
            '',
            f'ğŸ“ˆ æ¥½æ›²: {new_stats["total_songs"]}é¦–',
            f'ğŸ“Š è­œé¢: {new_stats["total_sheets"]}å€‹',
            '',
            '(åˆå›æ›´æ–°å®Œäº†ï¼)'
        ]

        return {
            'success': True,
            'new_stats': new_stats,
            'old_stats': None,
            'diff': None,
            'message': '\n'.join(message_parts)
        }
